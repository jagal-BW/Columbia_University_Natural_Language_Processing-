{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jason Galvan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/batman/Downloads'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "import random\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreSubs) > 0:\n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObjsFromConjunctions(objs):\n",
    "    moreObjs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreObjs) > 0:\n",
    "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
    "    return moreObjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVerbsFromConjunctions(verbs):\n",
    "    moreVerbs = []\n",
    "    for verb in verbs:\n",
    "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
    "            if len(moreVerbs) > 0:\n",
    "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
    "    return moreVerbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSubs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verbNegated = isNegated(head)\n",
    "            subs.extend(getSubsFromConjunctions(subs))\n",
    "            return subs, verbNegated\n",
    "        elif head.head != head:\n",
    "            return findSubs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], isNegated(tok)\n",
    "    return [], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSVs(tokens):\n",
    "    svs = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        if len(subs) > 0:\n",
    "            for sub in subs:\n",
    "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
    "    return svs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObjsFromPrepositions(deps):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
    "    return objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObjsFromAttrs(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(getObjsFromPrepositions(rights))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getObjFromXComp(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(getObjsFromPrepositions(rights))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllSubs(v):\n",
    "    verbNegated = isNegated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(getSubsFromConjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verbNegated = findSubs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verbNegated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllObjs(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    #potentialNewVerb, potentialNewObjs = getObjsFromAttrs(rights)\n",
    "    #if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "    #    objs.extend(potentialNewObjs)\n",
    "    #    v = potentialNewVerb\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSVOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjs(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    svos.append((sub.lower_, \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))\n",
    "    return svos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDeps(toks):\n",
    "    for tok in toks:\n",
    "        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n",
    "\n",
    "def testSVOs():\n",
    "    #nlp = English()\n",
    "\n",
    "    tok = nlp(\"In a surprise blog post, Amazon said it will put the brakes on providing its facial recognition technology to police for one year, but refuses to say if the move applies to federal law enforcement agencies.\"  \n",
    "              \"The moratorium comes two days after IBM said in a letter it was leaving the facial recognition market altogether.\" \n",
    "              \"Arvind Krishna, IBM's chief executive, cited a pursuit of justice and racial equity in light of the recent protests sparked by the killing of George Floyd by a white police officer in Minneapolis last month.\"  \n",
    "              \"Amazon's statement — just 102 words in length — did not say why it was putting the moratorium in place, but noted that Congress appears ready to work on stronger regulations governing the use of facial recognition — again without providing any details.\" \n",
    "              \"It's likely in response to the Justice in Policing Act, a bill that would, if passed, restrict how police can use facial recognition technology.\"\n",
    "              \"We hope this one-year moratorium might give Congress enough time to implement appropriate rules, and we stand ready to help if requested, said Amazon in the unbylined blog post.\"\n",
    "              \"But the statement did not say if the moratorium would apply to the federal government, the source of most of the criticism against Amazon's facial recognition technology.\" \n",
    "              \"Amazon also did not say in the statement what action it would take after the yearlong moratorium expires.\"\n",
    "              \"Amazon is known to have pitched its facial recognition technology, Rekognition, to federal agencies, like Immigration and Customs Enforcement.\" \n",
    "              \"Last year, Amazon's cloud chief Andy Jassy said in an interview the company would provide Rekognition to any government department.\"\n",
    "              \"Amazon spokesperson Kristin Brown declined to comment further or say if the moratorium applies to federal law enforcement.\"\n",
    "              \"There are dozens of companies providing facial recognition technology to police, but Amazon is by far the biggest.\" \n",
    "              \"Amazon has come under the most scrutiny after its Rekognition face-scanning technology showed bias against people of color.\")\n",
    "    svos = findSVOs(tok)\n",
    "    #printDeps(tok)\n",
    "    print(svos)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('it', 'put', 'brakes'), ('it', 'leaving', 'market'), ('krishna', 'cited', 'pursuit'), ('krishna', 'cited', 'equity'), ('it', 'putting', 'moratorium'), ('regulations', 'governing', 'use'), ('police', 'use', 'technology'), ('moratorium', 'give', 'congress'), ('moratorium', 'give', 'time'), ('time', 'implement', 'rules'), ('amazon', 'pitched', 'technology'), ('company', 'provide', 'rekognition'), ('company', 'provide', 'to'), ('companies', 'providing', 'technology'), ('technology', 'showed', 'bias')]\n"
     ]
    }
   ],
   "source": [
    "#Extract and print subject-verb-object (SVO) relations from each sentence \n",
    "if __name__ == \"__main__\":\n",
    "    testSVOs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrase_extractor = TextRank4Keyword()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon - 3.8802230107695324\n",
      "recognition - 3.063949530407874\n",
      "technology - 2.771828162901035\n",
      "police - 2.408977137971219\n",
      "moratorium - 2.352823648224532\n",
      "Rekognition - 1.8097523672725209\n",
      "year - 1.7093370318643593\n",
      "IBM - 1.3081532659520239\n",
      "Congress - 1.2540659965274186\n",
      "government - 1.2481422535786657\n",
      "light - 1.2223783339281367\n",
      "protests - 1.219482203432928\n"
     ]
    }
   ],
   "source": [
    "#Apply TextRank for ranking and selecting key phrases, print the result\n",
    "\n",
    "text = \"\"\"In a surprise blog post, Amazon said it will put the brakes on providing its facial recognition technology to police for one year, but refuses to say if the move applies to federal law enforcement agencies.\n",
    "\n",
    "The moratorium comes two days after IBM said in a letter it was leaving the facial recognition market altogether.\n",
    "\n",
    "Arvind Krishna, IBM's chief executive, cited a pursuit of justice and racial equity in light of the recent protests sparked by the killing of George Floyd by a white police officer in Minneapolis last month.\n",
    "\n",
    "Amazon's statement — just 102 words in length — did not say why it was putting the moratorium in place, but noted that Congress appears ready to work on stronger regulations governing the use of facial recognition — again without providing any details.\n",
    "\n",
    "It's likely in response to the Justice in Policing Act, a bill that would, if passed, restrict how police can use facial recognition technology.\n",
    "\n",
    "We hope this one-year moratorium might give Congress enough time to implement appropriate rules, and we stand ready to help if requested, said Amazon in the unbylined blog post.\n",
    "\n",
    "But the statement did not say if the moratorium would apply to the federal government, the source of most of the criticism against Amazon's facial recognition technology.\n",
    "\n",
    "Amazon also did not say in the statement what action it would take after the yearlong moratorium expires.\n",
    "\n",
    "Amazon is known to have pitched its facial recognition technology, Rekognition, to federal agencies, like Immigration and Customs Enforcement.\n",
    "\n",
    "Last year, Amazon's cloud chief Andy Jassy said in an interview the company would provide Rekognition to any government department.\n",
    "\n",
    "Amazon spokesperson Kristin Brown declined to comment further or say if the moratorium applies to federal law enforcement.\n",
    "\n",
    "There are dozens of companies providing facial recognition technology to police, but Amazon is by far the biggest. \n",
    "\n",
    "Amazon has come under the most scrutiny after its Rekognition face-scanning technology showed bias against people of color.\"\"\"\n",
    "\n",
    "\n",
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN',\"ADP\"], window_size=8, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another TextRank Implementation \n",
    "#!conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_chunks(text, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'):\n",
    "    import itertools, nltk, string\n",
    "    \n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in tagged_sents))\n",
    "    # join constituent chunk words into a single chunked phrase\n",
    "    candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
    "                  for key, group in itertools.groupby(all_chunks, lambda word__pos__chunk: word__pos__chunk[2] != 'O') if key]\n",
    "\n",
    "    return [cand for cand in candidates\n",
    "            if cand not in stop_words and not all(char in punct for char in cand)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
    "    import itertools, nltk, string\n",
    "\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
    "                                                                    for sent in nltk.sent_tokenize(text)))\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and not all(char in punct for char in word)]\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_keyphrases_by_tfidf(texts, candidates='chunks'):\n",
    "    import gensim, nltk\n",
    "    \n",
    "    # extract candidates from each text in texts, either chunks or words\n",
    "    if candidates == 'chunks':\n",
    "        boc_texts = [extract_candidate_chunks(text) for text in texts]\n",
    "    elif candidates == 'words':\n",
    "        boc_texts = [extract_candidate_words(text) for text in texts]\n",
    "    # make gensim dictionary and corpus\n",
    "    dictionary = gensim.corpora.Dictionary(boc_texts)\n",
    "    corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]\n",
    "    # transform corpus with tf*idf model\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    return corpus_tfidf, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_keyphrases_by_textrank(text, n_keywords=0.05):\n",
    "    from itertools import takewhile, tee\n",
    "    import operator\n",
    "    import networkx, nltk\n",
    "    \n",
    "    # tokenize for all words, and extract *candidate* words\n",
    "    words = [word.lower()\n",
    "             for sent in nltk.sent_tokenize(text)\n",
    "             for word in nltk.word_tokenize(sent)]\n",
    "    candidates = extract_candidate_words(text)\n",
    "    # build graph, each node is a unique candidate\n",
    "    graph = networkx.Graph()\n",
    "    graph.add_nodes_from(set(candidates))\n",
    "    # iterate over word-pairs, add unweighted edges into graph\n",
    "    def pairwise(iterable):\n",
    "        \"\"\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\"\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "    for w1, w2 in pairwise(candidates):\n",
    "        if w2:\n",
    "            graph.add_edge(*sorted([w1, w2]))\n",
    "    # score nodes using default pagerank algorithm, sort by score, keep top n_keywords\n",
    "    ranks = networkx.pagerank(graph)\n",
    "    if 0 < n_keywords < 1:\n",
    "        n_keywords = int(round(len(candidates) * n_keywords))\n",
    "    word_ranks = {word_rank[0]: word_rank[1]\n",
    "                  for word_rank in sorted(ranks.items(), key=operator.itemgetter(1), reverse=True)[:n_keywords]}\n",
    "                  #for word_rank in sorted(ranks.iteritems(), key=lambda x: x[1], reverse=True)[:n_keywords]}\n",
    "                  \n",
    "    #sorted(max_value_score.items(), key=operator.itemgetter(1), reverse=True)[:3]\n",
    "    keywords = set(word_ranks.keys())\n",
    "    # merge keywords into keyphrases\n",
    "    keyphrases = {}\n",
    "    j = 0\n",
    "    for i, word in enumerate(words):\n",
    "        if i < j:\n",
    "            continue\n",
    "        if word in keywords:\n",
    "            kp_words = list(takewhile(lambda x: x in keywords, words[i:i+10]))\n",
    "            avg_pagerank = sum(word_ranks[w] for w in kp_words) / float(len(kp_words))\n",
    "            keyphrases[' '.join(kp_words)] = avg_pagerank\n",
    "            # counter as hackish way to ensure merged keyphrases are non-overlapping\n",
    "            j = i + len(kp_words)\n",
    "            \n",
    "    return sorted(keyphrases.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    #return sorted(keyphrases.iteritems(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amazon', 0.05295206215281476),\n",
       " ('moratorium', 0.03853730582425931),\n",
       " ('facial', 0.02501702815608999),\n",
       " ('technology', 0.023647286518997324),\n",
       " ('—', 0.021017787716854034),\n",
       " ('rekognition', 0.02031813115085264),\n",
       " ('federal', 0.019365986155854005)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"In a surprise blog post, Amazon said it will put the brakes on providing its facial recognition technology to police for one year, but refuses to say if the move applies to federal law enforcement agencies.\n",
    "\n",
    "The moratorium comes two days after IBM said in a letter it was leaving the facial recognition market altogether.\n",
    "\n",
    "Arvind Krishna, IBM's chief executive, cited a pursuit of justice and racial equity in light of the recent protests sparked by the killing of George Floyd by a white police officer in Minneapolis last month.\n",
    "\n",
    "Amazon's statement — just 102 words in length — did not say why it was putting the moratorium in place, but noted that Congress appears ready to work on stronger regulations governing the use of facial recognition — again without providing any details.\n",
    "\n",
    "It's likely in response to the Justice in Policing Act, a bill that would, if passed, restrict how police can use facial recognition technology.\n",
    "\n",
    "We hope this one-year moratorium might give Congress enough time to implement appropriate rules, and we stand ready to help if requested, said Amazon in the unbylined blog post.\n",
    "\n",
    "But the statement did not say if the moratorium would apply to the federal government, the source of most of the criticism against Amazon's facial recognition technology.\n",
    "\n",
    "Amazon also did not say in the statement what action it would take after the yearlong moratorium expires.\n",
    "\n",
    "Amazon is known to have pitched its facial recognition technology, Rekognition, to federal agencies, like Immigration and Customs Enforcement.\n",
    "\n",
    "Last year, Amazon's cloud chief Andy Jassy said in an interview the company would provide Rekognition to any government department.\n",
    "\n",
    "Amazon spokesperson Kristin Brown declined to comment further or say if the moratorium applies to federal law enforcement.\n",
    "\n",
    "There are dozens of companies providing facial recognition technology to police, but Amazon is by far the biggest. \n",
    "\n",
    "Amazon has come under the most scrutiny after its Rekognition face-scanning technology showed bias against people of color.\"\"\"\n",
    "\n",
    "score_keyphrases_by_textrank(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sumy in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (0.8.1)\n",
      "Requirement already satisfied: requests>=2.7.0 in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from sumy) (2.22.0)\n",
      "Requirement already satisfied: breadability>=0.1.20 in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: nltk>=3.0.2 in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from sumy) (3.4.5)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from sumy) (20.7.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->sumy) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->sumy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->sumy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.7.0->sumy) (1.24.2)\n",
      "Requirement already satisfied: lxml>=2.0 in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from breadability>=0.1.20->sumy) (4.4.1)\n",
      "Requirement already satisfied: six in /Users/batman/opt/anaconda3/lib/python3.7/site-packages (from nltk>=3.0.2->sumy) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "class TextSummary(object):\n",
    "\n",
    "    def __init__(self, feeds_str, num_sents):\n",
    "        self.summary = str()\n",
    "        \n",
    "        parser = PlaintextParser.from_string(feeds_str, Tokenizer(\"english\"))\n",
    "        summarizer = LexRankSummarizer()\n",
    "\n",
    "        sentences = summarizer(parser.document, num_sents)  # Summarize the document with 5 sentences\n",
    "        for sentence in sentences:\n",
    "            self.summary += (sentence.__unicode__())\n",
    "\n",
    "    def output(self):\n",
    "        return self.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"In a surprise blog post, Amazon said it will put the brakes on providing its facial recognition technology to police for one year, but refuses to say if the move applies to federal law enforcement agencies.\n",
    "\n",
    "The moratorium comes two days after IBM said in a letter it was leaving the facial recognition market altogether.\n",
    "\n",
    "Arvind Krishna, IBM's chief executive, cited a pursuit of justice and racial equity in light of the recent protests sparked by the killing of George Floyd by a white police officer in Minneapolis last month.\n",
    "\n",
    "Amazon's statement — just 102 words in length — did not say why it was putting the moratorium in place, but noted that Congress appears ready to work on stronger regulations governing the use of facial recognition — again without providing any details.\n",
    "\n",
    "It's likely in response to the Justice in Policing Act, a bill that would, if passed, restrict how police can use facial recognition technology.\n",
    "\n",
    "We hope this one-year moratorium might give Congress enough time to implement appropriate rules, and we stand ready to help if requested, said Amazon in the unbylined blog post.\n",
    "\n",
    "But the statement did not say if the moratorium would apply to the federal government, the source of most of the criticism against Amazon's facial recognition technology.\n",
    "\n",
    "Amazon also did not say in the statement what action it would take after the yearlong moratorium expires.\n",
    "\n",
    "Amazon is known to have pitched its facial recognition technology, Rekognition, to federal agencies, like Immigration and Customs Enforcement.\n",
    "\n",
    "Last year, Amazon's cloud chief Andy Jassy said in an interview the company would provide Rekognition to any government department.\n",
    "\n",
    "Amazon spokesperson Kristin Brown declined to comment further or say if the moratorium applies to federal law enforcement.\n",
    "\n",
    "There are dozens of companies providing facial recognition technology to police, but Amazon is by far the biggest. \n",
    "\n",
    "Amazon has come under the most scrutiny after its Rekognition face-scanning technology showed bias against people of color.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a surprise blog post, Amazon said it will put the brakes on providing its facial recognition technology to police for one year, but refuses to say if the move applies to federal law enforcement agencies.The moratorium comes two days after IBM said in a letter it was leaving the facial recognition market altogether.Amazon's statement — just 102 words in length — did not say why it was putting the moratorium in place, but noted that Congress appears ready to work on stronger regulations governing the use of facial recognition — again without providing any details.But the statement did not say if the moratorium would apply to the federal government, the source of most of the criticism against Amazon's facial recognition technology.There are dozens of companies providing facial recognition technology to police, but Amazon is by far the biggest.\n"
     ]
    }
   ],
   "source": [
    "#Apply LexRank to produce an extractive summary of 5 sentences.\n",
    "text_to_sum = TextSummary(input_text,5)\n",
    "print(text_to_sum.output())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End of Assignment 5 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
