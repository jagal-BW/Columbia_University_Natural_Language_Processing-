{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jason Galvan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simhash in /Users/batman/anaconda3/lib/python3.7/site-packages (1.10.2)\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install webhoseio\n",
    "!pip install simhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webhoseio, os\n",
    "from gensim.models import KeyedVectors\n",
    "import json\n",
    "from simhash import Simhash, SimhashIndex\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3779\n"
     ]
    }
   ],
   "source": [
    "#Importing Saved Webhose Dataset\n",
    "\n",
    "json_data=open('/Users/batman/Downloads/webhose_IBM.json').readlines()\n",
    "newsfeeds_read = []\n",
    "for line in json_data:\n",
    "    newsfeeds_read.append(json.loads(line))\n",
    "print(len(newsfeeds_read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing Original Dataset by Ids/Titles Only ---placed # to prevent lengthly output \n",
    "feeds = []\n",
    "i = 0\n",
    "for feed in newsfeeds_read:\n",
    "    feed['id'] = i\n",
    "    #print(feed['id'], (feed['title']))  #Can unhash and code will print id + titles separate from remaining data \n",
    "    i += 1\n",
    "    feeds.append(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec Google News model...\n",
      "Finished loading Word2Vec Google News model...\n"
     ]
    }
   ],
   "source": [
    "model_path = '/Users/batman/Downloads/'\n",
    "def load_wordvec_model(modelName, modelFile, flagBin):\n",
    "    print('Loading ' + modelName + ' model...')\n",
    "    model = KeyedVectors.load_word2vec_format(model_path + modelFile, binary=flagBin)\n",
    "    print('Finished loading ' + modelName + ' model...')\n",
    "    return model\n",
    "\n",
    "model_w2v_AP    = load_wordvec_model('Word2Vec Google News', 'GoogleNews-vectors-negative300.bin.gz', True)\n",
    "#model_fasttext = load_wordvec_model('FastText', 'fastText_wiki_en.vec', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function checks whether the input words are present in the vocabulary for the model\n",
    "def vocab_check(vectors, words):\n",
    "    \n",
    "    output = list()\n",
    "    for word in words:\n",
    "        if word in vectors.vocab:\n",
    "            output.append(word.strip())\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculates similarity between two strings using a particular word vector model\n",
    "def calc_similarity(input1, input2, vectors):\n",
    "    s1words = set(vocab_check(vectors, input1.split()))\n",
    "    s2words = set(vocab_check(vectors, input2.split()))\n",
    "    #if len(s1words) < 1 | len(s2words) < 1:\n",
    "    #return 0\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        output = vectors.n_similarity(s1words, s2words)\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        output = 0\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(input):\n",
    "    # remove English stopwords\n",
    "    input = input.replace(\"'s\", \" \").replace(\"n’t\", \" not\").replace(\"’ve\", \" have\")\n",
    "    input = re.sub(r'[^a-zA-Z0-9 ]', '', input)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set distance to 5 because only using Simhash algorithm.  Simhashing requires a smaller distance because it \n",
    "#operates by splitting/hashing text into chunks.  If using w2vec, I would set distance higher--at approximately 20.  \n",
    "#W2vec rquires higher distance because it employs a vector method for comparison purposes, i.e. vectorizing words.  \n",
    "\n",
    "#I used several distance amounts and found that 5 produced the most accurate results. \n",
    "\n",
    "import logging\n",
    "logging.getLogger('simhash').setLevel(logging.CRITICAL) \n",
    "\n",
    "distance = 5\n",
    "objs = [(str(feed['id']), Simhash(str(feed['title']))) for feed in feeds]\n",
    "index = SimhashIndex(objs, k=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Designed Simhash alogrithm which saves duplicate ids/titles under variable dup_indices\n",
    "\n",
    "dup_indices = []\n",
    "dict1= dict()\n",
    "\n",
    "for id in range(len(feeds)):\n",
    "    dict1[id] = [id]\n",
    "    \n",
    "for j in range(len(feeds)):\n",
    "    feed_sel = feeds[j]\n",
    "    feed_hash = Simhash(str(feed_sel['title']))\n",
    "    indices = index.get_near_dups(feed_hash)\n",
    "    \n",
    "    for a in indices: \n",
    "        if (a not in dup_indices) and (a != j) and (j not in dict1[int(a)]):\n",
    "            dup_indices.append(a)\n",
    "        dict1[a] = dict1[int(a)].append(int(j))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3779"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Length of feeds ='s the total length of the entire Dataset with ALL ids/titles.  I am using this as a benchmark.  \n",
    "\n",
    "len(feeds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1285"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This variable ='s total amount of Duplicates identified by the Simhash Algorithm.  Our goal is to remove\n",
    "#the duplicates from the original dataset (feeds).   \n",
    "\n",
    "len(dup_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting Simhash Duplicate ids+ titles + text--Saved below as a Json file: \n",
    "dup_titles=[]\n",
    "\n",
    "for dupi in dup_indices:\n",
    "    dup_titles.append((feeds[int(dupi)]['id'], feeds[int(dupi)]['title'], feeds[int(dupi)]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting Simhash Duplicate ids + titles 'only' for Ease of Review--\n",
    "#Saved below as a Json file: \n",
    "\n",
    "dup_titles2=[]\n",
    "\n",
    "for dupi in dup_indices:\n",
    "    dup_titles2.append((feeds[int(dupi)]['id'], feeds[int(dupi)]['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1285"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dup_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1285"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dup_titles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting Webhose 'ids' +'titles' + 'text' to variable orig_titles--Saved below as a \n",
    "#Json file:\n",
    "\n",
    "orig_titles= []\n",
    "\n",
    "for feed in newsfeeds_read:\n",
    "    orig_titles.append((feed['id'], feed['title'], feed['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-3555f14da50c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morig_titles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0morig_titles2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "#Collecting Webhose only 'ids' and 'titles' to variable orig_titles--for Ease of Review\n",
    "#Saved below as a Json file:\n",
    "\n",
    "orig_titles2= []\n",
    "\n",
    "for feed in orig_titles:\n",
    "    orig_titles2.append((feed['title']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3779"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensuring length of orig_titles ='s length of feeds above (total titles in Webhose dataset)\n",
    "len(orig_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3779"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_titles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Removing duplicate titles from original Webhose dataset =deduplicated subset of titles---\n",
    "#Assignment Goal (leaves us with deduplicated ids + titles + text)\n",
    "\n",
    "dedup_titles = (set(orig_titles).difference(dup_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deduplicated ids + titles:  for ease of review \n",
    "\n",
    "dedup_titles2 = (set(orig_titles2).difference(dup_titles2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2494"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensuring duplicate titles were removed:  total dataset titles (3779) - duplicate titles \n",
    "#(1285) ='s 2494 titles\n",
    "len(dedup_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2903"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dedup_titles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting deduplicated subset or ids + titles + text--\n",
    "#Saved below as a Json file:\n",
    "\n",
    "sorted_dedupTitles = sorted(dedup_titles, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting deduplicated subset or ids + titles for ease of review--\n",
    "#Saved below as a Json file:\n",
    "sorted_dedupTitles2 = sorted(dedup_titles2, reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2494"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensuring sorted deduplicated subset is proper length (2494):\n",
    "\n",
    "len(sorted_dedupTitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2494"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_dedupTitles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving all Original Webhose Ids/Titles to json file: \n",
    "\n",
    "with open ('/Users/batman/Downloads/originalTitles.json', \"w\") as data_file:\n",
    "    for x in orig_titles2:\n",
    "        line = json.dumps(x)\n",
    "        data_file.write(line)\n",
    "        data_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Deduplicated Subset of Ids/Titles/Text to json file ---Non-Sorted: Assignment Goal\n",
    "\n",
    "with open ('/Users/batman/Downloads/deduplicatedData.json', \"w\") as data_file:\n",
    "    for x in dedup_titles:\n",
    "        line = json.dumps(x)\n",
    "        data_file.write(line)\n",
    "        data_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Deduplicated Subset of Ids/Titles (only) to json file ---Non-Sorted: \n",
    "with open ('/Users/batman/Downloads/deduplicatedTitles.json', \"w\") as data_file:\n",
    "    for x in dedup_titles2:\n",
    "        line = json.dumps(x)\n",
    "        data_file.write(line)\n",
    "        data_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Sorted Deduplicated Subset of Ids/Titles/Text to json file---Sorted:  \n",
    "\n",
    "with open ('/Users/batman/Downloads/sorted_deduplicatedData.json', \"w\") as data_file:\n",
    "    for x in sorted_dedupTitles:\n",
    "        line = json.dumps(x)\n",
    "        data_file.write(line)\n",
    "        data_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Sorted Deduplicated Subset of Ids/Titles to json file---Sorted:\n",
    "with open ('/Users/batman/Downloads/sorted_deduplicatedTitles.json', \"w\") as data_file:\n",
    "    for x in sorted_dedupTitles2:\n",
    "        line = json.dumps(x)\n",
    "        data_file.write(line)\n",
    "        data_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Simhash Duplicate Ids/Titles/Text--for Reference.  I did not want to print above as it is \n",
    "#1285 articles....very long.  So decided to save into a json file for review.   \n",
    "\n",
    "with open ('/Users/batman/Downloads/duplicateData.json', \"w\") as data_file:\n",
    "    for j in dup_titles:\n",
    "        line = json.dumps(j)\n",
    "        data_file.write(line)\n",
    "        data_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Saving Simhash Duplicate Ids/Titles--for Reference.\n",
    "\n",
    "with open ('/Users/batman/Downloads/duplicateTitles.json', \"w\") as data_file:\n",
    "    for j in dup_titles2:\n",
    "        line = json.dumps(j)\n",
    "        data_file.write(line)\n",
    "        data_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading JSON file back into Python array of JSON objects and confirming total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "json_data=open('/Users/batman/Downloads/deduplicatedData.json').readlines()\n",
    "dedup_subset = []\n",
    "for line in json_data:\n",
    "    dedup_subset.append(json.loads(line))\n",
    "print(len(dedup_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End of Assignment 7 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
